{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyzZ1Ib2CM3Z",
        "outputId": "3c89bde1-898e-41ec-d216-16d2cf35daf0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: requests in e:\\lab6\\venv\\lib\\site-packages (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in e:\\lab6\\venv\\lib\\site-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in e:\\lab6\\venv\\lib\\site-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\lab6\\venv\\lib\\site-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in e:\\lab6\\venv\\lib\\site-packages (from requests) (2025.7.14)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "%pip install requests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjkBwMCJCtcg"
      },
      "source": [
        "## Step 1: Document Collection\n",
        "\n",
        "In this step, we will simulate a small document corpus (text dataset) consisting of a few short text samples. These documents will be used to build our Inverted Index.\n",
        "\n",
        "**Talking Point 1:** A good test document collection should contain varied vocabulary and overlapping terms to verify token mapping accuracy in the inverted index.\n",
        "\n",
        "**Talking Point 2:** For real-world use, text may come from files, web scraping, or databases ‚Äî but we‚Äôll keep it simple here using plain strings in memory.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAe8u7R8C0iP",
        "outputId": "355d267f-aa25-45f5-aab1-b7c1ede6f462"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample doc ID: doc_0\n",
            "\n",
            "I was wondering if anyone out there could enlighten me on this car I saw\n",
            "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
            "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
            "the front bumper was separate from the rest of the body. This is \n",
            "all I know. If anyone can tellme a model name, engine specs, years\n",
            "of production, where this car is made, history, or whatever info you\n",
            "have on this funky looking car, please e-mail....\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import random\n",
        "\n",
        "# Load a subset of the dataset for quick processing\n",
        "newsgroups_data = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "# We'll take only the first 20 documents for this workshop\n",
        "documents = newsgroups_data.data[:20]\n",
        "\n",
        "# Assign unique IDs to each document\n",
        "document_dict = {f'doc_{i}': doc for i, doc in enumerate(documents)}\n",
        "\n",
        "# Preview first document\n",
        "print(f\"Sample doc ID: doc_0\\n\\n{document_dict['doc_0'][:500]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqisIaRJDMum"
      },
      "source": [
        "##  Step 2: Tokenizer Implementation\n",
        "\n",
        "Talking Point 2 (Markdown):\n",
        "A tokenizer breaks raw text into words. A custom tokenizer gives us flexibility to handle domain-specific patterns, punctuation, and edge cases. For example, we might want to keep hyphenated words or handle contractions differently.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nC0RrjriDPYN",
        "outputId": "2b821cda-2390-48de-e808-393e82db8500"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['i', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'i', 'saw', 'the', 'other', 'day', 'it', 'was']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def tokenize(text):\n",
        "    \"\"\"\n",
        "    Tokenizes the input text into words.\n",
        "    - Lowercases\n",
        "    - Removes punctuation\n",
        "    - Splits on whitespace\n",
        "    \"\"\"\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "    # Remove punctuation using regex\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    # Split into tokens\n",
        "    tokens = text.split()\n",
        "    return tokens\n",
        "\n",
        "# Example usage on one document\n",
        "sample_tokens = tokenize(document_dict['doc_0'])\n",
        "print(sample_tokens[:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k32eU5S-DrcH"
      },
      "source": [
        "## Step 3: Normalization Pipeline\n",
        " Text normalization (like removing stop words and stemming) reduces noise and improves IR accuracy. For example, \"running\", \"runs\", and \"ran\" all reduce to \"run\" ‚Äî helping the Inverted Index map them to the same concept."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9SLFgUmD-ca",
        "outputId": "9def9275-0457-42ce-ec84-718c20b0b897"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in e:\\lab6\\venv\\lib\\site-packages (3.9.1)\n",
            "Requirement already satisfied: click in e:\\lab6\\venv\\lib\\site-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in e:\\lab6\\venv\\lib\\site-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in e:\\lab6\\venv\\lib\\site-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in e:\\lab6\\venv\\lib\\site-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: colorama in e:\\lab6\\venv\\lib\\site-packages (from click->nltk) (0.4.6)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "['wonder', 'anyon', 'could', 'enlighten', 'car', 'saw', 'day', '2door', 'sport', 'car', 'look', 'late', '60', 'earli', '70', 'call', 'bricklin', 'door', 'realli', 'small']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\shiru\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "%pip install nltk\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Initialize stopwords and stemmer\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def normalize(tokens):\n",
        "    \"\"\"\n",
        "    Normalizes a list of tokens by:\n",
        "    - Removing stop words\n",
        "    - Applying stemming\n",
        "    \"\"\"\n",
        "    normalized = []\n",
        "    for token in tokens:\n",
        "        if token not in stop_words:\n",
        "            stemmed = stemmer.stem(token)\n",
        "            normalized.append(stemmed)\n",
        "    return normalized\n",
        "\n",
        "# Example: Normalize a sample document\n",
        "tokens = tokenize(document_dict['doc_0'])\n",
        "normalized_tokens = normalize(tokens)\n",
        "\n",
        "print(normalized_tokens[:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2s7ByqwECGu"
      },
      "source": [
        "# Step 4: Build and Query the Inverted Index\n",
        "An Inverted Index maps each term to a list of documents it appears in. This makes search extremely efficient, especially at scale. It's the backbone of search engines and information retrieval systems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "7d8irDoKENuP"
      },
      "outputs": [],
      "source": [
        "#Build the inverted Index\n",
        "inverted_index = {\n",
        "    \"term1\": {\"doc_3\", \"doc_7\"},\n",
        "    \"term2\": {\"doc_1\", \"doc_4\", \"doc_9\"},\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBEXZPjQEYqT",
        "outputId": "eaccbf69-89c9-40fb-9a64-98af30ec4cd8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "wonder: ['doc_0', 'doc_2']\n",
            "anyon: ['doc_0', 'doc_16', 'doc_18']\n",
            "could: ['doc_0', 'doc_11', 'doc_17', 'doc_2']\n",
            "enlighten: ['doc_0']\n",
            "car: ['doc_0', 'doc_17']\n",
            "saw: ['doc_0']\n",
            "day: ['doc_0', 'doc_1', 'doc_13']\n",
            "2door: ['doc_0']\n",
            "sport: ['doc_0', 'doc_17']\n",
            "look: ['doc_0', 'doc_13', 'doc_15', 'doc_2']\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def build_inverted_index(documents):\n",
        "    \"\"\"\n",
        "    Builds an inverted index from the input dictionary of documents.\n",
        "    \"\"\"\n",
        "    index = defaultdict(set)\n",
        "\n",
        "    for doc_id, text in documents.items():\n",
        "        tokens = tokenize(text)\n",
        "        normalized_tokens = normalize(tokens)\n",
        "\n",
        "        for token in normalized_tokens:\n",
        "            index[token].add(doc_id)\n",
        "\n",
        "    return index\n",
        "\n",
        "# Build and store the inverted index\n",
        "inverted_index = build_inverted_index(document_dict)\n",
        "\n",
        "# Preview sample entries\n",
        "for i, (term, doc_ids) in enumerate(inverted_index.items()):\n",
        "    print(f\"{term}: {sorted(list(doc_ids))}\")\n",
        "    if i == 9:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_RXzQYNEZiP"
      },
      "source": [
        "# Phrase Query Function\n",
        "We'll implement a basic query function that:\n",
        "\n",
        "Normalizes input query\n",
        "\n",
        "Finds documents containing all tokens (AND operation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfuhC6eSIx4z",
        "outputId": "d7e4ddd3-ead5-4b29-9ad1-3fc9f824c337"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Results for 'computer science': []\n",
            "Results for 'space nasa mission': []\n"
          ]
        }
      ],
      "source": [
        "def search(query, index):\n",
        "    \"\"\"\n",
        "    Search for documents that contain **all words** in the query.\n",
        "    \"\"\"\n",
        "    query_tokens = normalize(tokenize(query))\n",
        "\n",
        "    if not query_tokens:\n",
        "        return []\n",
        "\n",
        "    # Initialize result with docs containing the first token\n",
        "    result_docs = index.get(query_tokens[0], set()).copy()\n",
        "\n",
        "    # Intersect with other tokens' doc sets\n",
        "    for token in query_tokens[1:]:\n",
        "        result_docs &= index.get(token, set())\n",
        "\n",
        "    return sorted(result_docs)\n",
        "\n",
        "# üîç Run 2 Phrase Queries\n",
        "query1 = \"computer science\"\n",
        "query2 = \"space nasa mission\"\n",
        "\n",
        "print(f\"\\nResults for '{query1}': {search(query1, inverted_index)}\")\n",
        "print(f\"Results for '{query2}': {search(query2, inverted_index)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45lVj0_sFVpD",
        "outputId": "6c4fa9ef-aa38-4d82-8a56-fc0dfb962008"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query 1 normalized: ['comput', 'scienc']\n",
            "Query 2 normalized: ['space', 'nasa', 'mission']\n"
          ]
        }
      ],
      "source": [
        "print(\"Query 1 normalized:\", normalize(tokenize(\"computer science\")))\n",
        "print(\"Query 2 normalized:\", normalize(tokenize(\"space nasa mission\")))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbsJF4SXGw--",
        "outputId": "eec10d0f-753f-421b-9cd1-1ec695ea1d1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Docs with 'comput': {'doc_13', 'doc_2'}\n",
            "Docs with 'scienc': []\n",
            "Docs with 'space': {'doc_13'}\n",
            "Docs with 'nasa': []\n",
            "Docs with 'mission': {'doc_13'}\n"
          ]
        }
      ],
      "source": [
        "# Query 1: 'comput', 'scienc'\n",
        "print(\"Docs with 'comput':\", inverted_index.get('comput', []))\n",
        "print(\"Docs with 'scienc':\", inverted_index.get('scienc', []))\n",
        "\n",
        "# Query 2: 'space', 'nasa', 'mission'\n",
        "print(\"Docs with 'space':\", inverted_index.get('space', []))\n",
        "print(\"Docs with 'nasa':\", inverted_index.get('nasa', []))\n",
        "print(\"Docs with 'mission':\", inverted_index.get('mission', []))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6_vV8GiHcnV",
        "outputId": "ba8aa830-05b5-491f-b589-d00172cc809a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OR Search - computer science: ['doc_13', 'doc_2']\n",
            "OR Search - space nasa mission: ['doc_13']\n"
          ]
        }
      ],
      "source": [
        "def search_or(query, index):\n",
        "    query_tokens = normalize(tokenize(query))\n",
        "    result_docs = set()\n",
        "    for token in query_tokens:\n",
        "        result_docs |= index.get(token, set())\n",
        "    return sorted(result_docs)\n",
        "\n",
        "print(\"OR Search - computer science:\", search_or(\"computer science\", inverted_index))\n",
        "print(\"OR Search - space nasa mission:\", search_or(\"space nasa mission\", inverted_index))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aw8xWi0CH0r4"
      },
      "source": [
        "## Phrase Query Tests\n",
        "\n",
        "*Talking Point 5:* Phrase queries using AND logic are strict ‚Äî all terms must exist in the same document. If even one is missing, the query returns no result. This highlights how token coverage impacts IR accuracy.\n",
        "```python\n",
        "# AND Queries\n",
        "print(\"Search (AND) - 'space mission':\", search(\"space mission\", inverted_index))\n",
        "print(\"Search (AND) - 'comput mission':\", search(\"comput mission\", inverted_index))\n",
        "\n",
        "# OR Queries\n",
        "print(\"Search (OR) - 'computer science':\", search_or(\"computer science\", inverted_index))\n",
        "print(\"Search (OR) - 'space nasa mission':\", search_or(\"space nasa mission\", inverted_index))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3yJ0d4UPJX6"
      },
      "source": [
        "# 5. Stemming ‚Äì Applied PorterStemmer\n",
        "Add Stemming Using NLTK‚Äôs PorterStemmer\n",
        "Here‚Äôs exactly what to add to your existing normalize() function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZjA3nMyPDwl",
        "outputId": "f9186324-6b20-4fb5-8213-ac2f215c3072"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\shiru\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "# Download required resources\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Initialize stemmer and stopword list\n",
        "stemmer = PorterStemmer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Normalization function\n",
        "def normalize(tokens):\n",
        "    normalized = []\n",
        "    for token in tokens:\n",
        "        token = token.lower().strip(string.punctuation)\n",
        "        if token and token not in stop_words:\n",
        "            stemmed = stemmer.stem(token)\n",
        "            normalized.append(stemmed)\n",
        "    return normalized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "wVsbb_YTPzUV"
      },
      "outputs": [],
      "source": [
        "def tokenizer_for_vectorizer(text):\n",
        "    return normalize(tokenize(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yja9R8pPP2up",
        "outputId": "faed5b29-33dd-4a05-afb7-d82ff7312e03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After stemming: ['comput', 'run', 'happili', 'drive', 'mice']\n"
          ]
        }
      ],
      "source": [
        "tokens = [\"computers\", \"running\", \"happily\", \"drives\", \"mice\"]\n",
        "print(\"After stemming:\", normalize(tokens))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnaP31fhLowF"
      },
      "source": [
        "# Step 6: Vectorization and Cosine Similarity\n",
        "Vectorization converts processed text into numerical form so we can compute similarity between documents. We use TF-IDF to give higher weight to rare but meaningful terms. Cosine similarity helps identify how similar two documents are in terms of their content, regardless of their length.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFvlPihBL5DL",
        "outputId": "773e4b59-d854-45e4-92bb-ae10a71013d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cosine Similarity between doc_0 and other docs:\n",
            "doc_0 vs doc_0: 1.0000\n",
            "doc_0 vs doc_1: 0.0124\n",
            "doc_0 vs doc_2: 0.0538\n",
            "doc_0 vs doc_3: 0.0000\n",
            "doc_0 vs doc_4: 0.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "e:\\Lab6\\venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Reuse the same documents (we'll only use 5 here for quick testing)\n",
        "sample_docs = [document_dict[f'doc_{i}'] for i in range(5)]\n",
        "\n",
        "# Initialize vectorizer with preprocessing\n",
        "vectorizer = TfidfVectorizer(\n",
        "    tokenizer=tokenize,         # Custom tokenizer\n",
        "    stop_words='english',       # Built-in stopword removal\n",
        "    lowercase=True              # Lowercasing\n",
        ")\n",
        "\n",
        "# Transform documents into TF-IDF matrix\n",
        "tfidf_matrix = vectorizer.fit_transform(sample_docs)\n",
        "\n",
        "# Compute cosine similarity matrix (doc-to-doc)\n",
        "cosine_sim_matrix = cosine_similarity(tfidf_matrix)\n",
        "\n",
        "# Display similarity between doc_0 and others\n",
        "print(\"Cosine Similarity between doc_0 and other docs:\")\n",
        "for i in range(len(sample_docs)):\n",
        "    print(f\"doc_0 vs doc_{i}: {cosine_sim_matrix[0][i]:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-FaI-PrOVlc",
        "outputId": "a7ff66df-a7a0-4c5c-e129-87b6a4560dc1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TF-IDF Vocabulary: ['wondering', 'enlighten', 'car', 'saw', 'day', '2door', 'sports', 'looked', 'late', '60s', 'early', '70s', 'called', 'bricklin', 'doors', 'really', 'small', 'addition', 'bumper', 'separate', 'rest', 'body', 'know', 'tellme', 'model', 'engine', 'specs', 'years', 'production', 'history', 'info', 'funky', 'looking', 'email', 'fair', 'number', 'brave', 'souls', 'upgraded', 'si', 'clock', 'oscillator', 'shared', 'experiences', 'poll', 'send', 'brief', 'message', 'detailing', 'procedure', 'speed', 'attained', 'cpu', 'rated', 'add', 'cards', 'adapters', 'heat', 'sinks', 'hour', 'usage', 'floppy', 'disk', 'functionality', '800', '14', 'm', 'floppies', 'especially', 'requested', 'summarizing', 'days', 'network', 'knowledge', 'base', 'upgrade', 'havent', 'answered', 'thanks', 'folks', 'mac', 'plus', 'finally', 'gave', 'ghost', 'weekend', 'starting', 'life', '512k', 'way', '1985', 'sooo', 'im', 'market', 'new', 'machine', 'bit', 'sooner', 'intended', 'picking', 'powerbook', '160', 'maybe', '180', 'bunch', 'questions', 'hopefully', 'somebody', 'answer', 'does', 'anybody', 'dirt', 'round', 'introductions', 'expected', 'id', 'heard', '185c', 'supposed', 'make', 'appearence', 'summer', 'anymore', 'dont', 'access', 'macleak', 'rumors', 'price', 'drops', 'line', 'like', 'ones', 'duos', 'just', 'went', 'recently', 'whats', 'impression', 'display', 'probably', 'swing', 'got', '80mb', '120', 'feel', 'better', 'yea', 'looks', 'great', 'store', 'wow', 'good', 'solicit', 'opinions', 'people', 'use', 'daytoday', 'worth', 'taking', 'size', 'money', 'hit', 'active', 'realize', 'real', 'subjective', 'question', 'ive', 'played', 'machines', 'computer', 'breifly', 'figured', 'actually', 'uses', 'daily', 'prove', 'helpful', 'hellcats', 'perform', 'advance', 'ill', 'post', 'summary', 'news', 'reading', 'time', 'premium', 'finals', 'corner', 'tom', 'willis', 'twillisecnpurdueedu', 'purdue', 'electrical', 'engineering', 'weiteks', 'addressphone', 'information', 'chip', 'article', 'c5owcbn3pworldstdcom', 'tombakerworldstdcom', 'baker', 'understanding', 'errors', 'basically', 'known', 'bugs', 'warning', 'software', 'things', 'checked', 'right', 'values', 'arent', 'set', 'till', 'launch', 'suchlike', 'fix', 'code', 'possibly', 'introduce', 'tell', 'crew', 'ok', '213', 'liftoff', 'ignore']\n"
          ]
        }
      ],
      "source": [
        "print(\"TF-IDF Vocabulary:\", list(vectorizer.vocabulary_.keys()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrQEEuKuMGRQ"
      },
      "source": [
        "# Query Test (TF-IDF + Cosine):\n",
        "We now test the similarity of a user query like \"space shuttle program\" to each of the documents and rank them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZQvYB_CMQPs",
        "outputId": "24338b06-d4a1-4b1f-c8f9-bd884854cb21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Query: 'floppy disk'\n",
            "Ranked documents (doc_id, similarity score):\n",
            "doc_1: 0.1669\n",
            "doc_2: 0.0714\n",
            "doc_0: 0.0000\n",
            "doc_3: 0.0000\n",
            "doc_4: 0.0000\n"
          ]
        }
      ],
      "source": [
        "query = [\"floppy disk\"]\n",
        "query_vec = vectorizer.transform(query)\n",
        "similarities = cosine_similarity(query_vec, tfidf_matrix).flatten()\n",
        "\n",
        "ranked_docs = sorted(zip(range(len(sample_docs)), similarities), key=lambda x: -x[1])\n",
        "\n",
        "print(f\"\\nQuery: '{query[0]}'\")\n",
        "print(\"Ranked documents (doc_id, similarity score):\")\n",
        "for doc_id, score in ranked_docs:\n",
        "    print(f\"doc_{doc_id}: {score:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
